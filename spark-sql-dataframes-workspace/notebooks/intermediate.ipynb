{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://robsons-imac.lan:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Intermediate</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fde12b59a00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Intermediate\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('json').load('/Users/rob_the_programmer/Documents/programming/2022/hands_on_data_science/spark-sql-dataframes-workspace/files/utlization.json')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cpu_utilization: double (nullable = true)\n",
      " |-- event_datetime: string (nullable = true)\n",
      " |-- free_memory: double (nullable = true)\n",
      " |-- server_id: long (nullable = true)\n",
      " |-- session_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp view created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.createOrReplaceTempView(\"utilization\")\n",
    "    print(\"Temp view created successfully\")\n",
    "except:\n",
    "    print(\"Error creating temp view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsql = spark.sql(\"SELECT * FROM utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      100|           47|\n",
      "|      100|           43|\n",
      "|      100|           62|\n",
      "|      100|           50|\n",
      "|      100|           43|\n",
      "|      100|           48|\n",
      "|      100|           58|\n",
      "|      100|           58|\n",
      "|      100|           62|\n",
      "|      100|           45|\n",
      "|      100|           47|\n",
      "|      100|           60|\n",
      "|      100|           57|\n",
      "|      100|           44|\n",
      "|      100|           47|\n",
      "|      100|           66|\n",
      "|      100|           65|\n",
      "|      100|           66|\n",
      "|      100|           42|\n",
      "|      100|           63|\n",
      "+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('select server_id, session_count from utilization')\n",
    "dfsql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid| sc|\n",
      "+---+---+\n",
      "|100| 47|\n",
      "|100| 43|\n",
      "|100| 62|\n",
      "|100| 50|\n",
      "|100| 43|\n",
      "|100| 48|\n",
      "|100| 58|\n",
      "|100| 58|\n",
      "|100| 62|\n",
      "|100| 45|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('''\n",
    "    select\n",
    "        server_id as sid,\n",
    "        session_count as sc\n",
    "        from utilization\n",
    "        limit 10\n",
    "''')\n",
    "dfsql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.66|03/05/2019 08:06:48|       0.31|      120|           54|\n",
      "|           0.58|03/05/2019 08:11:48|       0.38|      120|           64|\n",
      "|           0.55|03/05/2019 08:16:48|       0.61|      120|           54|\n",
      "|            0.7|03/05/2019 08:21:48|       0.35|      120|           80|\n",
      "|            0.6|03/05/2019 08:26:48|       0.39|      120|           71|\n",
      "|           0.53|03/05/2019 08:31:48|       0.35|      120|           49|\n",
      "|           0.73|03/05/2019 08:36:48|       0.42|      120|           73|\n",
      "|           0.41|03/05/2019 08:41:48|        0.6|      120|           72|\n",
      "|           0.62|03/05/2019 08:46:48|       0.57|      120|           57|\n",
      "|           0.67|03/05/2019 08:51:48|       0.44|      120|           78|\n",
      "|           0.67|03/05/2019 08:56:48|       0.38|      120|           73|\n",
      "|           0.39|03/05/2019 09:01:48|       0.47|      120|           58|\n",
      "|            0.5|03/05/2019 09:06:48|       0.29|      120|           78|\n",
      "|           0.38|03/05/2019 09:11:48|       0.38|      120|           56|\n",
      "|           0.53|03/05/2019 09:16:48|       0.38|      120|           47|\n",
      "|           0.74|03/05/2019 09:21:48|       0.25|      120|           69|\n",
      "|           0.53|03/05/2019 09:26:48|       0.57|      120|           73|\n",
      "|           0.54|03/05/2019 09:31:48|       0.64|      120|           65|\n",
      "|            0.7|03/05/2019 09:36:48|       0.52|      120|           55|\n",
      "|           0.54|03/05/2019 09:41:48|       0.27|      120|           74|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('select * from utilization where server_id = 120')\n",
    "dfsql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid| sc|\n",
      "+---+---+\n",
      "|100| 71|\n",
      "|100| 71|\n",
      "|100| 71|\n",
      "|100| 71|\n",
      "|100| 72|\n",
      "|100| 72|\n",
      "|100| 71|\n",
      "|100| 71|\n",
      "|100| 71|\n",
      "|100| 72|\n",
      "|100| 71|\n",
      "|100| 72|\n",
      "|100| 71|\n",
      "|100| 71|\n",
      "|100| 72|\n",
      "|100| 71|\n",
      "|100| 72|\n",
      "|100| 72|\n",
      "|100| 71|\n",
      "|100| 71|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('select server_id as sid, session_count as sc from utilization where session_count > 70')\n",
    "dfsql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "239659"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid| sc|\n",
      "+---+---+\n",
      "|120| 80|\n",
      "|120| 71|\n",
      "|120| 73|\n",
      "|120| 72|\n",
      "|120| 78|\n",
      "|120| 73|\n",
      "|120| 78|\n",
      "|120| 73|\n",
      "|120| 74|\n",
      "|120| 78|\n",
      "|120| 75|\n",
      "|120| 75|\n",
      "|120| 73|\n",
      "|120| 79|\n",
      "|120| 72|\n",
      "|120| 77|\n",
      "|120| 75|\n",
      "|120| 72|\n",
      "|120| 79|\n",
      "|120| 75|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('''\n",
    "\n",
    "    select \n",
    "        server_id as sid,\n",
    "        session_count as sc\n",
    "    from \n",
    "        utilization\n",
    "    where\n",
    "        session_count > 70\n",
    "    and\n",
    "        server_id =120\n",
    "    ''')\n",
    "\n",
    "dfsql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid| sc|\n",
      "+---+---+\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "|120| 80|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('''\n",
    "\n",
    "    select \n",
    "        server_id as sid,\n",
    "        session_count as sc\n",
    "    from \n",
    "        utilization\n",
    "    where\n",
    "        session_count > 70\n",
    "    and\n",
    "        server_id =120\n",
    "    order by 2 desc\n",
    "    ''')\n",
    "\n",
    "dfsql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregating dataframes with sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sid: bigint, sc: bigint]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('select * from utilization')\n",
    "dfsql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  500000|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('select count(*) from utilization')\n",
    "dfsql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  239659|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('select count(*) from utilization where session_count > 70')\n",
    "dfsql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      112|    7425|\n",
      "|      113|    9418|\n",
      "|      110|    2826|\n",
      "|      107|    5646|\n",
      "|      103|    8744|\n",
      "|      104|    7366|\n",
      "|      111|    3093|\n",
      "|      100|     391|\n",
      "|      105|    1110|\n",
      "|      108|    8375|\n",
      "+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('select server_id, count(*) from utilization where session_count > 70 group by server_id')\n",
    "dfsql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      101|    9808|\n",
      "|      113|    9418|\n",
      "|      145|    9304|\n",
      "|      103|    8744|\n",
      "|      102|    8586|\n",
      "|      133|    8583|\n",
      "|      108|    8375|\n",
      "|      149|    8288|\n",
      "|      137|    8248|\n",
      "|      148|    8027|\n",
      "+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('select server_id, count(*) from utilization where session_count > 70 group by server_id order by 2 desc')\n",
    "dfsql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------------+-----------------+\n",
      "|server_id|min_session_count|avg_session_count|max_session_count|\n",
      "+---------+-----------------+-----------------+-----------------+\n",
      "|      101|               71|87.66557911908646|              105|\n",
      "|      113|               71|86.96262476109577|              103|\n",
      "|      145|               71|86.97732158211522|              103|\n",
      "|      103|               71|85.76372369624886|              101|\n",
      "|      102|               71|85.71150710458886|              101|\n",
      "|      133|               71|85.46720260981009|              100|\n",
      "|      108|               71| 85.1219104477612|              100|\n",
      "|      149|               71| 84.9612693050193|               99|\n",
      "|      137|               71| 85.0061833171678|               99|\n",
      "|      148|               71|84.70350068518749|               99|\n",
      "|      123|               71|84.53220510229856|               98|\n",
      "|      118|               71|84.65777833944142|               98|\n",
      "|      112|               71|83.54505050505051|               97|\n",
      "|      139|               71|83.32710280373831|               96|\n",
      "|      104|               71|83.34604941623677|               96|\n",
      "|      142|               71| 82.9049971767363|               95|\n",
      "|      121|               71|82.88848108413326|               95|\n",
      "|      146|               71|82.95093325791855|               95|\n",
      "|      126|               71|81.56150824823253|               93|\n",
      "|      144|               71|81.38472668810289|               92|\n",
      "+---------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('''\n",
    "    select \n",
    "        server_id, \n",
    "        min(session_count) as min_session_count, \n",
    "        avg(session_count) as avg_session_count,\n",
    "        max(session_count) as max_session_count\n",
    "    from utilization\n",
    "    where\n",
    "        session_count > 70\n",
    "    group by server_id\n",
    "    order by count(*) desc\n",
    "''')\n",
    "\n",
    "dfsql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------------+-----------------+\n",
      "|server_id|min_session_count|avg_session_count|max_session_count|\n",
      "+---------+-----------------+-----------------+-----------------+\n",
      "|      101|               71|             88.0|              105|\n",
      "|      113|               71|             87.0|              103|\n",
      "|      145|               71|             87.0|              103|\n",
      "|      103|               71|             86.0|              101|\n",
      "|      102|               71|             86.0|              101|\n",
      "|      133|               71|             85.0|              100|\n",
      "|      108|               71|             85.0|              100|\n",
      "|      149|               71|             85.0|               99|\n",
      "|      137|               71|             85.0|               99|\n",
      "|      148|               71|             85.0|               99|\n",
      "|      123|               71|             85.0|               98|\n",
      "|      118|               71|             85.0|               98|\n",
      "|      112|               71|             84.0|               97|\n",
      "|      139|               71|             83.0|               96|\n",
      "|      104|               71|             83.0|               96|\n",
      "|      142|               71|             83.0|               95|\n",
      "|      121|               71|             83.0|               95|\n",
      "|      146|               71|             83.0|               95|\n",
      "|      126|               71|             82.0|               93|\n",
      "|      144|               71|             81.0|               92|\n",
      "+---------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql('''\n",
    "    select \n",
    "        server_id, \n",
    "        min(session_count) as min_session_count, \n",
    "        round(avg(session_count)) as avg_session_count,\n",
    "        max(session_count) as max_session_count\n",
    "    from utilization\n",
    "    where\n",
    "        session_count > 70\n",
    "    group by server_id\n",
    "    order by count(*) desc\n",
    "''')\n",
    "\n",
    "dfsql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joining dataframes with sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfutil = spark.read.format('json').load('../files/utlization.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n",
      "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|\n",
      "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|\n",
      "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|\n",
      "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|\n",
      "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n",
      "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|\n",
      "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|\n",
      "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|\n",
      "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfutil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfutil.createOrReplaceTempView(\"util\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfserver = spark.read.csv('../files/server_name.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "|      102| 102 Server|\n",
      "|      103| 103 Server|\n",
      "|      104| 104 Server|\n",
      "|      105| 105 Server|\n",
      "|      106| 106 Server|\n",
      "|      107| 107 Server|\n",
      "|      108| 108 Server|\n",
      "|      109| 109 Server|\n",
      "+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfserver.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfserver.createOrReplaceTempView(\"server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|server_id|\n",
      "+---------+\n",
      "|      100|\n",
      "|      101|\n",
      "|      102|\n",
      "|      103|\n",
      "|      104|\n",
      "|      105|\n",
      "|      106|\n",
      "|      107|\n",
      "|      108|\n",
      "|      109|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfcount = spark.sql('select distinct(server_id) from util order by server_id')\n",
    "dfcount.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "|      102| 102 Server|\n",
      "|      103| 103 Server|\n",
      "|      104| 104 Server|\n",
      "|      105| 105 Server|\n",
      "|      106| 106 Server|\n",
      "|      107| 107 Server|\n",
      "|      108| 108 Server|\n",
      "|      109| 109 Server|\n",
      "+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from server').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|server_id|server_name|session_count|\n",
      "+---------+-----------+-------------+\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           43|\n",
      "|      100| 100 Server|           62|\n",
      "|      100| 100 Server|           50|\n",
      "|      100| 100 Server|           43|\n",
      "|      100| 100 Server|           48|\n",
      "|      100| 100 Server|           58|\n",
      "|      100| 100 Server|           58|\n",
      "|      100| 100 Server|           62|\n",
      "|      100| 100 Server|           45|\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           60|\n",
      "|      100| 100 Server|           57|\n",
      "|      100| 100 Server|           44|\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           66|\n",
      "|      100| 100 Server|           65|\n",
      "|      100| 100 Server|           66|\n",
      "|      100| 100 Server|           42|\n",
      "|      100| 100 Server|           63|\n",
      "+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select \n",
    "        u.server_id,\n",
    "        s.server_name,\n",
    "        u.session_count\n",
    "    from\n",
    "        util u\n",
    "    inner join\n",
    "        server s\n",
    "    on\n",
    "        u.server_id = s.server_id\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de-duplicating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import lit # lit stands for literal\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "dfdup = sc.parallelize([\n",
    "                        Row(server_name='101 Server', cpu_utilization=85, session_count=80), \n",
    "                        Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                        Row(server_name='102 Server', cpu_utilization=85, session_count=80),\n",
    "                        Row(server_name='102 Server', cpu_utilization=85, session_count=80\n",
    "                    )]).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           80|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfdup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfdup.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfdup.drop_duplicates(['server_name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.parallelize([\n",
    "                        Row(server_name='101 Server', cpu_utilization=85, session_count=80), \n",
    "                        Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                        Row(server_name='102 Server', cpu_utilization=85, session_count=40),\n",
    "                        Row(server_name='103 Server', cpu_utilization=70, session_count=80),\n",
    "                        Row(server_name='104 Server', cpu_utilization=60, session_count=80)\n",
    "                    ]).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           40|\n",
      "| 103 Server|             70|           80|\n",
      "| 104 Server|             60|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|  null|\n",
      "| 101 Server|             80|           90|  null|\n",
      "| 102 Server|             85|           40|  null|\n",
      "| 103 Server|             70|           80|  null|\n",
      "| 104 Server|             60|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfna = df.withColumn('na_col', lit(None).cast(StringType()))\n",
    "dfna.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     A|\n",
      "| 101 Server|             80|           90|     A|\n",
      "| 102 Server|             85|           40|     A|\n",
      "| 103 Server|             70|           80|     A|\n",
      "| 104 Server|             60|           80|     A|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfna.fillna('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     A|\n",
      "| 101 Server|             80|           90|     A|\n",
      "| 102 Server|             85|           40|     A|\n",
      "| 103 Server|             70|           80|     A|\n",
      "| 104 Server|             60|           80|     A|\n",
      "| 101 Server|             85|           80|  null|\n",
      "| 101 Server|             80|           90|  null|\n",
      "| 102 Server|             85|           40|  null|\n",
      "| 103 Server|             70|           80|  null|\n",
      "| 104 Server|             60|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = dfna.fillna('A').union(dfna)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     A|\n",
      "| 101 Server|             80|           90|     A|\n",
      "| 102 Server|             85|           40|     A|\n",
      "| 103 Server|             70|           80|     A|\n",
      "| 104 Server|             60|           80|     A|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('natable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     A|\n",
      "| 101 Server|             80|           90|     A|\n",
      "| 102 Server|             85|           40|     A|\n",
      "| 103 Server|             70|           80|     A|\n",
      "| 104 Server|             60|           80|     A|\n",
      "| 101 Server|             85|           80|  null|\n",
      "| 101 Server|             80|           90|  null|\n",
      "| 102 Server|             85|           40|  null|\n",
      "| 103 Server|             70|           80|  null|\n",
      "| 104 Server|             60|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from natable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|  null|\n",
      "| 101 Server|             80|           90|  null|\n",
      "| 102 Server|             85|           40|  null|\n",
      "| 103 Server|             70|           80|  null|\n",
      "| 104 Server|             60|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from natable where na_col is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     A|\n",
      "| 101 Server|             80|           90|     A|\n",
      "| 102 Server|             85|           40|     A|\n",
      "| 103 Server|             70|           80|     A|\n",
      "| 104 Server|             60|           80|     A|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from natable where na_col is not null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "843ed4014e0c5c9016b0be3f012b8ea5a0b4055cbc7abd39c785865cd85ff6da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
